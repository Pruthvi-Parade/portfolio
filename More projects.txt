________________________________________
Drone Security Analyst Agent Prototype
Pruthvi Parade
13-04-25
________________________________________
1. Introduction / Executive Summary
This report details the design, implementation, and evaluation of a prototype Drone Security Analyst Agent, developed in response to the AI Engineer assignment. The primary goal was to create a system capable of processing simulated drone telemetry and video feeds to automatically monitor a fixed property, detect security-relevant events, log findings, generate alerts, and provide a queryable history of observations.
The developed prototype successfully meets these requirements by leveraging modern AI and software engineering techniques. It utilizes Python as the primary language, OpenCV for video frame ingestion, a pre-trained Object Detection model (DETR via Hugging Face Transformers) for analyzing visual content, LangChain for orchestrating the processing workflow, SQLite for persistent indexing of detection results, and OpenAI's GPT-3.5-turbo model (via LangChain) to implement a natural language Question-Answering capability based on the indexed data.
The prototype demonstrates the core functionalities: automated processing of video frames, identification and logging of detected objects (persons, cars, etc.), generation of basic alerts based on detection rules, persistent storage of results, and the ability to query the detection history using natural language. This report outlines the system architecture, design choices, implementation details, testing methodology, results achieved, the critical role of AI assistance during development, encountered challenges, and potential future enhancements.
________________________________________
2. System Architecture & Design
The system is designed with a modular architecture to ensure separation of concerns and facilitate potential future expansion. The core components and data flow are orchestrated using the LangChain framework within the main agent class.
2.1. Core Components:
•	Simulator (src/simulator.py): Reads video frames sequentially from a standard video file (data/sample_video.mp4) using OpenCV. It simulates basic telemetry by associating a timestamp and a static location ("Main Street Intersection" in the demo) with each yielded frame. Frame skipping is implemented to manage processing load.
•	Object Detector (src/vlm_processor.py - ObjectDetector class): Encapsulates the object detection logic. It loads the facebook/detr-resnet-50 model and associated processor using the Hugging Face transformers library. Its primary method, detect_objects, takes a raw video frame (NumPy array) and returns a list of detected objects, including their class label (e.g., 'person', 'car'), confidence score, and bounding box coordinates.
•	Indexer (src/indexer.py - FrameIndexerSQLiteOD class): Manages the persistent storage of analysis results. It uses Python's built-in sqlite3 module to interact with a database file (data/security_analysis_od.db). The key table (detections) stores the timestamp, location, frame number, and the list of detected objects (including labels, scores, boxes) for each frame, serialized as a JSON string. It provides methods to add new detection records and query existing records based on criteria like time range, location, and object labels.
•	Agent (src/agent.py - DroneSecurityAgentOD class): The central orchestrator.
o	Initializes and holds instances of the simulator, detector, and indexer.
o	Defines and manages two primary LangChain sequences:
	Object Detection Processing Chain: Takes input frame data, calls the ObjectDetector, and passes the results to a final processing function (_process_frame_result_od) which handles logging, alerting, and indexing.
	Question-Answering (Q&A) Chain: Initializes ChatOpenAI (GPT-3.5-turbo), defines a custom retriever function (_retrieve_context_for_qa) that queries the SQLite database based on keywords extracted from the user's question, formats the retrieved detection data as context, and uses a prompt template to instruct the LLM to answer the question based only on the provided context.
o	Implements helper methods for logging detected objects (_log_event_od) and checking alert conditions (_check_alerts_od).
o	Provides the main run method to execute the frame processing loop and the answer_question method to interact with the Q&A chain.
2.2. System Flow Diagram:
 
    
2.3. Data Flow Summary:
1.	The Simulator yields frames and basic telemetry.
2.	The Agent receives this data and passes it to the LangChain OD processing sequence.
3.	The sequence invokes the ObjectDetector to get structured detection results.
4.	The results are passed to the agent's processing function, which:
o	Prints formatted LOG messages to the console.
o	Checks detection labels against predefined rules and prints ALERT messages if triggered.
o	Calls the Indexer to save the timestamp, location, and JSON representation of the detections list to the SQLite database.
5.	For Q&A, the user provides a question to the Agent.
6.	The agent's _retrieve_context_for_qa function extracts keywords, queries the Indexer's database, and formats relevant detection records into a context string.
7.	The Q&A chain passes the question and the context string to the ChatOpenAI LLM via a specific prompt.
8.	The LLM generates an answer based solely on the provided context.
9.	The agent returns the final answer string.
________________________________________
3. Assumptions Made
Several assumptions were made during the development of this prototype:
•	Simulated Environment: The provided sample video and simulated location telemetry were assumed to be representative enough for demonstrating core functionality. Real-world drone data might have varying quality, viewpoints, and more complex telemetry.
•	Fixed Camera View: The prototype assumes a relatively fixed camera viewpoint for the duration of the analysis; it does not explicitly handle significant drone movement or gimbal changes that would require more complex spatial reasoning.
•	Object Detector Suitability: The pre-trained facebook/detr-resnet-50 model was assumed to be adequate for detecting common objects (persons, cars) without requiring fine-tuning on domain-specific data. Its performance and the range of detectable objects are limited by its training data.
•	Contextual Scope: The analysis primarily focuses on individual frames. Complex temporal events requiring object tracking or analysis across extended time periods (e.g., accurately determining "loitering" vs. "waiting briefly") are not implemented.
•	Rule Simplicity: Alert rules are based on the simple presence of detected object labels within a single frame. More sophisticated rules involving object interactions, zones, or temporal patterns were outside the scope.
•	Performance: Real-time processing constraints were relaxed. Frame skipping was used, and the processing time per frame (especially object detection on CPU) may not meet strict real-time requirements.
•	LLM Access: The Q&A feature assumes the availability and correct configuration of an OpenAI API key for accessing GPT-3.5-turbo.
________________________________________
4. Justification of Tools & Configurations
The selection of tools aimed to balance modern AI capabilities, development efficiency, and alignment with the assignment's goals:
•	Python: Chosen for its extensive ecosystem of libraries for AI/ML (Transformers, PyTorch), data handling (Pandas - though not heavily used here), web frameworks (Streamlit - attempted), and general-purpose programming.
•	OpenCV (opencv-python): The industry standard for computer vision tasks, providing robust and efficient video file reading and basic image manipulation capabilities.
•	Hugging Face transformers & DETR: Provides easy access to state-of-the-art pre-trained models. DETR (facebook/detr-resnet-50) was selected as a representative transformer-based object detection model offering a good balance between performance and ease of integration compared to captioning models for extracting structured security information.
•	SQLite: Chosen for data persistence due to its simplicity (serverless, built into Python), ease of setup, and adequate performance for storing and querying structured detection data at a prototype scale. It avoids the overhead of setting up external database servers. Storing detections as JSON strings is a pragmatic approach for this scale.
•	LangChain: Adopted to structure the application logic into modular chains, facilitating the orchestration of multiple steps (detection -> logging/alerting/indexing) and the separate, complex flow of the Q&A feature (retrieval -> prompt -> LLM -> parse). It promotes code organization and makes future extensions (like adding more complex agents or memory) easier.
•	OpenAI GPT-3.5-turbo (ChatOpenAI): Selected for the Q&A component due to its strong natural language understanding and ability to follow instructions (i.e., answer based only on provided context), readily available through the LangChain integration.
•	Configuration Choices:
o	Frame Skipping: A necessary trade-off to make processing feasible within reasonable timeframes, especially on CPU hardware, allowing demonstration of the end-to-end flow without excessive waiting.
o	Detection Confidence Threshold: Set initially at 0.85 (adjustable) to filter out low-confidence detections, aiming for higher precision in reported objects at the potential cost of missing some lower-confidence ones.
________________________________________
5. Implementation Details & Results
5.1. Key Implementation Aspects:
•	Object Detection Integration: The ObjectDetector class successfully loads the DETR model and uses AutoImageProcessor and post_process_object_detection to generate structured outputs.
•	Database Schema: The SQLite detections table stores timestamp, location, frame_number, and the detected_objects list (serialized as JSON).
•	LangChain Orchestration: The agent.py defines two distinct chains using RunnablePassthrough, RunnableLambda, RunnableParallel, PromptTemplate, ChatOpenAI, and StrOutputParser to manage the OD processing and Q&A flows.
•	Q&A Retrieval: A custom retriever function (_retrieve_context_for_qa) performs basic keyword extraction and filtering on data fetched from the SQLite database to provide context to the LLM.
5.2. Results & Demonstration:
The prototype successfully processes the sample video, performs object detection, logs results, triggers alerts, indexes data, and answers questions.
•	Object Detection & Logging: The console output during agent.run() demonstrates successful detection frame-by-frame.
o	Example Log Snippet:
o	      LOG [2025-04-12 23:13:33] Event at Main Street Intersection. Detections: [car(0.93), person(0.88)].
•	Alerting: Alerts were triggered based on defined rules.
o	Example Alert Snippet:
o	      ALERT [2025-04-12 23:13:08] Person detected at Main Street Intersection!
•	Indexing: Data was successfully indexed into the SQLite database (security_analysis_od.db), confirmed by the absence of indexing errors during the run and the final indexed frame count.
•	Question Answering: The agent demonstrated its ability to answer natural language questions based on the indexed data.
o	Example Q&A Snippet:
o	      QA - Answering question: 'Tell me about detected suitcases'
o	QA - Retrieving context for question: 'Tell me about detected suitcases'
o	  -> Identified object keywords: ['suitcases']. Matching singular labels: {'suitcase'}
o	  -> Found 1 frames containing labels: {'suitcase'}
o	  -> Retrieved context (showing first 500 chars):
o	Frames Analysis (Most Recent Relevant First):
o	- Time: 2025-04-12 23:13:21, Location: Main Street Intersection, Detections: [suitcase(score=0.88)]
o	
o	QA - Generated Answer: At 2025-04-12 23:13:21 at the Main Street Intersection, a suitcase was detected with a score of 0.88.
o	
o	Q: Tell me about detected suitcases
o	A: At 2025-04-12 23:13:21 at the Main Street Intersection, a suitcase was detected with a score of 0.88.
•	Video Demonstration: The functionality described above is visually demonstrated in the accompanying screen recordings (See Appendix for links). Video 1 shows the setup and live processing, while Video 2 focuses on the Q&A interaction.
________________________________________
6. AI Assistance Details
6.1. AI Tools Used:
•	Research & Concepts: Grok 3
•	Coding & Debugging: Gemini 2.5 Pro
•	Documentation: Claude
•	Core Application AI: facebook/detr-resnet-50, gpt-3.5-turbo

This project leveraged multiple AI tools, each playing a distinct role in the development process, alongside the core AI models used within the application itself.
Primary AI Models Used in the Application:
•	facebook/detr-resnet-50 (Object Detection): Loaded via Hugging Face transformers for analyzing video frames.
•	gpt-3.5-turbo (Language Model): Accessed via LangChain ChatOpenAI for the Question-Answering feature.
AI Assistant Tools Used During Development:
1.	Grok 3 (Deep Research & Concept Exploration):
o	Utilized Grok 3 for initial research into relevant technologies for drone-based security analysis, VLM/Object Detection models, database options for indexing time-series/structured data, and agentic frameworks like LangChain.
o	Specifically used Grok to compare DETR performance characteristics vs YOLOv8 for the object detection task within the context of Hugging Face transformers integration.
o	Explored different architectural patterns and potential challenges using Grok's research capabilities.
o	Investigated specific model details and library comparisons.
2.	Gemini 2.5 Pro (Coding Assistance & Debugging):
o	Leveraged Gemini primarily for code generation, implementation guidance, and debugging, often using context or summaries derived from Grok's research.
o	Code Generation: Provided prompts to generate boilerplate code for Python classes (ObjectDetector, FrameIndexerSQLiteOD, DroneSecurityAgentOD), specific functions (e.g., database interaction with sqlite3, LangChain sequence structures). Asked Gemini specifically for the LangChain RunnableParallel structure required for the Q&A chain implementation.
o	Implementation Guidance: Asked for examples on how to use specific libraries like Hugging Face transformers (loading models, post-processing DETR outputs), sqlite3 (JSON handling, querying), and LangChain (chain construction, retriever logic).
o	Debugging: Collaborated with Gemini to diagnose and fix runtime errors, including ModuleNotFoundError (relative imports), AttributeError (method name mismatches), TypeError (incorrect function arguments like in add_detections), and logical errors in the Q&A retriever's keyword/label matching. Provided error messages and code snippets to Gemini for analysis and suggested fixes.
3.	Claude (Documentation & Explanation):
o	Utilized Claude for assistance in drafting and refining documentation, particularly the explanatory sections of this README and the final report.
o	Provided Claude with descriptions of the project's architecture, components, workflow, and AI integrations. For instance, had Claude rewrite the System Architecture section based on a bulleted list of components and data flows provided.
o	Used Claude to help structure the documentation, rephrase technical concepts for clarity, and ensure comprehensive coverage of the assignment requirements in the written materials.
Impact on Workflow: This multi-tool approach significantly streamlined development:
•	Grok accelerated the initial research and technology comparison phase.
•	Gemini acted as an efficient pair programmer, translating requirements into functional Python code, providing specific LangChain structures, and rapidly resolving implementation bugs.
•	Claude facilitated the creation of clear and well-structured documentation based on technical inputs.
•	This division of labor allowed for focused use of each AI's strengths, leading to a more robust prototype and comprehensive documentation within the project timeframe.

6.2. Development Process Narrative:
•	Initial Research (Grok): Describe how Grok helped compare technologies (e.g., DETR vs YOLO), understand concepts (VLMs, LangChain agents), and explore potential database solutions. Mention specific comparisons made.
•	Prototyping & Code Generation (Gemini): Explain how you prompted Gemini for specific code structures (classes for detector/indexer/agent), library usage examples (Transformers, SQLite, LangChain Runnables), and function logic (e.g., post-processing DETR, Q&A context formatting). Give examples like the RunnableParallel structure prompt.
•	Integration (Gemini): Detail how Gemini assisted in connecting the different components – making the simulator output compatible with the detector input, structuring the LangChain sequence to pass data correctly, ensuring the agent called the indexer properly.
•	Debugging (Gemini): Provide specific examples of errors encountered (ModuleNotFound, AttributeError, TypeError, NameError, logical errors in retriever) and how you worked with Gemini (providing errors/code) to arrive at the fixes (relative imports, method name corrections, argument passing fixes, keyword mapping).
•	Documentation (Claude): Explain how you provided technical details or bullet points to Claude and requested it to generate or refine specific sections of the README or this report (e.g., System Architecture description).
6.3. Impact:
Summarize the tangible benefits – saved time, ability to implement more complex features (OD + Q&A), improved code structure, faster learning of new libraries (LangChain, Transformers), enhanced documentation quality. Emphasize how this iterative collaboration with AI tools was key to achieving the project goals.
________________________________________
7. Challenges Encountered & Limitations
While the prototype successfully demonstrates the core concepts, several challenges and limitations were noted:
•	Object Detector Accuracy/Relevance: The pre-trained DETR model, while functional, did not always provide highly accurate or relevant detections for every frame of the specific sample video (e.g., detecting umbrellas/suitcases prominently). Fine-tuning on domain-specific data or using a different OD model might yield better security-relevant results.
•	Q&A Retriever Simplicity: The current keyword-based retriever (_retrieve_context_for_qa) is basic. It can fail to identify keywords correctly (initially struggled with plurals) and doesn't perform semantic matching. This led to cases where relevant data existed in the index but wasn't provided as context to the LLM, resulting in factually incorrect answers (e.g., stating no people were detected when they were indexed).
•	Lack of Temporal Context: The system analyzes frames independently. It cannot track objects over time, making it difficult to implement rules based on duration (loitering), speed, direction, or counting distinct events (e.g., "car entered twice").
•	Alert Rule Simplicity: The rules are basic checks for the presence of specific object labels. They don't incorporate location zones (beyond the simulated overall location), time durations, object combinations (beyond simple examples), or confidence scores effectively.
•	Performance: Object detection inference, especially on a CPU, is time-consuming, limiting the real-time applicability without significant optimization or hardware acceleration (GPU). Frame skipping was necessary for practical demonstration.
•	Environment Sensitivity: Initial attempts to add a Streamlit UI encountered environment-specific conflicts between Streamlit's file watcher and PyTorch, highlighting potential deployment complexities.
________________________________________
8. Potential Improvements & Future Work
Based on the current limitations, several avenues exist for future enhancement:
•	Improve Detection:
o	Experiment with different, potentially larger or more specialized, object detection models (e.g., YOLO variants, Owl-ViT).
o	Fine-tune the chosen model on a dataset more representative of the target security environment.
o	Incorporate image pre-processing or enhancement techniques.
•	Enhance Q&A Retrieval:
o	Implement embedding-based semantic search. Store embeddings for detected objects/frame summaries in a vector store (e.g., ChromaDB, FAISS) integrated with SQLite. Use similarity search between the question embedding and document embeddings for retrieval.
o	Use an LLM to extract structured information (entities, intents) from the question to build more targeted database queries.
•	Implement Temporal Analysis & Object Tracking: Integrate object tracking algorithms (e.g., DeepSORT, SORT) to follow objects across frames. This would enable:
o	More robust loitering detection based on duration in an area.
o	Counting unique object entries/exits.
o	Rules based on object speed and trajectory.
•	Develop a Sophisticated Rule Engine: Abstract rules into a configurable format (YAML, JSON) or use a dedicated rule engine library. Allow rules based on zones, time, object combinations, confidence levels, and tracking data.
•	User Interface: Develop a dedicated web interface (potentially revisiting Streamlit or using Flask/Django) to display detections visually (bounding boxes on frames), manage alerts, configure rules, and provide an interactive Q&A prompt.
•	Performance Optimization: Utilize GPU acceleration, explore model quantization or lighter model architectures, optimize database queries.
•	Expand Telemetry Usage: Incorporate more detailed telemetry like drone altitude, heading, and GPS coordinates to provide richer spatial context for analysis and rules.
________________________________________
9. Conclusion
This project successfully delivered a functional prototype of a Drone Security Analyst Agent. By integrating OpenCV for video processing, a Hugging Face Object Detection model (DETR) for visual analysis, LangChain for workflow orchestration and Q&A, SQLite for persistent data indexing, and OpenAI's GPT-3.5 for natural language understanding, the prototype demonstrates a viable approach to automated security monitoring.
Key achievements include the successful detection and logging of objects in video frames, the generation of basic alerts based on detection rules, the creation of a queryable historical index of detections, and the implementation of a natural language Q&A interface. The development process heavily leveraged AI assistance tools (Grok, Gemini, Claude) for research, coding, debugging, and documentation, showcasing the potential for AI-accelerated development of complex systems.
While limitations exist, particularly regarding temporal analysis and the sophistication of the Q&A retriever, the prototype serves as a strong foundation. It validates the core architecture and demonstrates the potential for AI to significantly enhance situational awareness and security analysis capabilities in drone monitoring applications. Future work can build upon this foundation to address the identified limitations and create a more robust and feature-rich system.
________________________________________
10. Appendix
•	GitHub Repository: https://github.com/Pruthvi-Parade/aegis-drone-ai.git









------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------










Project 2 




Objective
•	Test the core features (mission planning, fleet management, real-time monitoring, survey reporting) to ensure they work.
•	Document the project in a README with setup instructions, design decisions, and AI tool usage.
•	Outline a demo video script to showcase the app.
Instructions
1.	Manual Testing 
o	Mission Planning: 
	Go to http://localhost:3000.
	Fill out the form (e.g., Name: "Test Mission 2", Area: "South Campus", Waypoints: "20,30", Start Time: future date).
	Click "Save Mission" and verify the success alert.
	Check /api/missions to see the new mission.
o	Fleet Management: 
	Visit http://localhost:3000/fleet.
	Confirm the table shows 3 drones (Drone 1, Drone 2, Drone 3) with their statuses and battery levels.
o	Real-Time Monitoring: 
	Set a mission to "in-progress" (e.g., ID 2): 
	Go to http://localhost:3000/monitor/2.
	Verify progress updates from 0% to 100% (every 2 seconds) and test Pause, Resume, Abort buttons.
	Note: If WebSocket isn’t working, try and simulated locally.
o	Survey Reporting: 
	Set a mission to "completed" (e.g., ID 1): 
	Visit http://localhost:3000/reports.
	Confirm the table lists completed missions with stats






2.	Create README File 
o	In the root directory, create README.md:
# Drone Survey Management System (DSMS)

A basic web app for planning, managing, and monitoring drone surveys, built for the FlytBase Full Stack Dev Code Assignment 2025.

## Setup Instructions
1. **Clone the Repository:**
   git clone https://github.com/Pruthvi-Parade/dsms
   cd dsms
2.	Install Dependencies: 
npm install
3.	Run the App: 
	Start Next.js and WebSocket servers: 
npm run start
	Or just Next.js 
npm run dev
4.	Access the App: 
	Open http://localhost:3000 in your browser.
Features
	Mission Planning: Create missions with name, area, waypoints, and start time.
	Fleet Management: View 3 hardcoded drones with status and battery levels.
	Real-Time Monitoring: Simulate mission progress with pause/resume/abort controls (WebSocket on port 3001).
	Survey Reporting: Display completed mission summaries.





Design Decisions
	Next.js: Chosen for simplicity, combining frontend and backend in one framework.
	shadcn/ui: Used for a polished UI with minimal effort.
	SQLite: Lightweight database for basic data storage.
	WebSocket (ws): Simple real-time updates, though faced ESM issues (resolved with CommonJS as fallback if needed).
	No Authentication: Skipped per assignment scope to keep it basic.
AI Tool Usage & Impact
1.	Tech Stack Selection & Planning
o	Used Grok & Claude for brainstorming the tech stack and structuring the development approach.
o	Helped break down implementation steps logically, reducing planning time by ~2 hours.
2.	Code Generation & Acceleration
o	Leveraged Grok for generating boilerplate code, including form structures and WebSocket logic.
o	Accelerated development by ~30%, minimizing repetitive coding tasks.
3.	Debugging & Issue Resolution
o	Utilized Grok for troubleshooting TypeScript ESM issues in WebSockets.
o	Provided quick solutions and optimizations, cutting down debugging time by ~1 hour.
4.	Code Refinement & Best Practices
o	Used AI to suggest improvements in code structure, modularization, and best practices.
o	Enhanced overall code readability and maintainability.

Known Issues
	WebSocket may not connect reliably due to ESM configuration challenges; simulated progress works locally with manual status updates.




Running the Demo
	Plan a mission at /.
	View fleet at /fleet.
	Monitor a mission at /monitor
	You may also use monitor/id in my web-socket branch to view individual drone
	See reports at /dashboard (set status to "completed" first).


